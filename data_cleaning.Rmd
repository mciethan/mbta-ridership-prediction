---
title: "Data Cleaning"
author: "Emmett Greenberg, Ted Banken, Ethan McIntosh"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytransit)
```

This script will house any data cleaning processes to go from raw data to model-ready intermediate outputs. These processes are in their own script file so that we don't want to do them every time we build a model.

## MBTA ridership data (dependent variable)

```{r}
# these data come from the MBTA open data portal. Link for humans: https://mbta-massdot.opendata.arcgis.com/search?q=rail%20ridership%20by%20time%20period 

ridership_f23 <- read.csv('https://hub.arcgis.com/api/v3/datasets/80a379ebaa374cfd836ca4d3880ceda4_0/downloads/data?format=csv&spatialRefId=4326&where=1%3D1')

ridership_f17_to_f19 <- read.csv('data/mbta/rail_ridership_stops_F17toF19.csv')
```

```{r}
f23_clean <- ridership_f23 %>% 
  group_by(season, route_id, parent_station) %>%
  summarise(across(c(total_ons, total_offs, number_service_days), sum), .groups='drop') %>%
  mutate(
    avg_boardings = total_ons/number_service_days, 
    avg_alightings = total_offs/number_service_days,
    .keep='unused'
    )

f19_clean <- ridership_f17_to_f19 %>% 
  filter(season=='Fall 2019') %>% 
  mutate(parent_station = stop_id) %>%
  group_by(season, route_id, parent_station) %>%
  summarise(across(c(total_ons, total_offs, number_service_days), sum), .groups='drop') %>%
  mutate(
    avg_boardings = total_ons/number_service_days, 
    avg_alightings = total_offs/number_service_days,
    .keep='unused'
    )

f19_clean %>% write.csv('data/cleaned/stations_routes_f19_ridership.csv')
f23_clean %>% write.csv('data/cleaned/stations_routes_f23_ridership.csv')
```

The unique identifier per ridership row is the combination of route_id and parent_station (meaning, for example, that State Street has separate rows for the Orange Line and the Blue Line). This informs how we process the various independent variables.

# MBTA GTFS

GTFS is a good canonical source for the lat/long coordinates for each station. GTFS is also probably the best source for calculating the number (and names) of bus routes that serve a given RT station. 

It is also an option to calculate scheduled travel times and headways from GTFS, as an alternative to using data on actual headways and travel times.

GTFS feeds have files larger than the GitHub size limit, so they're not included in the repo. Follow these instructions to run this part of the analysis:

1. Download the MBTA GTFS recap dataset from this link: https://www.arcgis.com/home/item.html?id=9ab1dc7ea2bf4ad7b7e25cc6b941b39a

2. Run this next cell to interactively choose the folder where you downloaded the dataset:

```{r}
downloaded_gtfs_path <- choose.dir(default = "", caption = "Select the GTFS recaps folder:")
```

I am not sure whether this path syntax will work on non-Windows machines.

```{r}
f23_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2023-fall-prerating-recap.zip'))
f19_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2019-fall-prerating-recap.zip'))
```

## select stops from stop_times on trips belonging to Rapid Transit routes

```{r}
rt_stations_per_route <- function(gtfs) { 
  gtfs$routes %>%
    filter(route_desc == 'Rapid Transit') %>% # filter to Rapid Transit routes
    inner_join(gtfs$trips, by='route_id') %>%
    inner_join(gtfs$stop_times, by='trip_id') %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    # reduce the Green Line's branches into a single Green route_id, consistent with ridership
    mutate(route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)) %>%
    select(route_id, route_long_name, parent_station) %>%
    # this second join to the stops table is so that we use the parent station IDs and their more
    # generalized coordinates instead of the more specific coordinates of each platform
    inner_join(gtfs$stops, by=join_by(parent_station==stop_id)) %>%
    select(route_id, parent_station, stop_name, stop_lat, stop_lon) %>%
    unique() # this selection lists each stop once per route they serve
}

rt_stations_f23 <- rt_stations_per_route(f23_gtfs)
rt_stations_f19 <- rt_stations_per_route(f19_gtfs)
rt_stations_f23
```
Save the station-route level location and name information to file

```{r}
rt_stations_f23 %>% write.csv('data/cleaned/stations_routes_f23.csv', row.names=FALSE)
rt_stations_f19 %>% write.csv('data/cleaned/stations_routes_f19.csv', row.names=FALSE)
```

## Calculate the average inbound weekday headway in seconds per route per station 

```{r}
library(hms)

headways_per_station_route <- function(gtfs) { 
  gtfs$stop_times %>%
    filter(departure_time > parse_hms('06:00:00'), departure_time < parse_hms('22:00:00'))  %>%
    inner_join(gtfs$trips, by='trip_id') %>%
    filter(direction_id == '1') %>% # only consider a single direction (inbound, in this case)
    inner_join(gtfs$routes, by='route_id') %>%
    filter(route_desc == 'Rapid Transit') %>%
    inner_join(gtfs$calendar_attributes, by='service_id') %>%
    filter(
      service_schedule_type == 'Weekday', # only consider weekday service
      service_schedule_typicality == 1) %>% # filter out atypical (e.g. special event) services
    mutate(direction_id = as.character(direction_id)) %>% # data type fix to avoid errors on the next join
    inner_join(gtfs$route_patterns, by=c('route_pattern_id', 'route_id', 'direction_id')) %>%
    filter(route_pattern_typicality == 1) %>% # filter out atypical (e.g. early morning) route variants
    inner_join(gtfs$stops, by='stop_id') %>%
    mutate(route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)) %>%
    group_by(route_id, parent_station) %>%
    # avg headway = total time elapsed divided by total number of departures during the period
    summarise(avg_headway = round(16*3600/n()), service_count = n_distinct(service_id), .groups='drop') 
}

f23_headways <- f23_gtfs %>% headways_per_station_route()
f19_headways <- f19_gtfs %>% headways_per_station_route()
```

```{r}
# this diagnostic checks that all the headways were calculated with a single service ID
# if there are values other than 1 in this output, the headways may need recalculation
f23_headways %>%
  select(service_count) %>%
  unique()

f19_headways %>%
  select(service_count) %>%
  unique()
```

```{r}
f23_headways %>% select(-service_count) %>% 
  write.csv('data/cleaned/stations_routes_f23_headways.csv', row.names=FALSE)
f19_headways %>% select(-service_count) %>% 
  write.csv('data/cleaned/stations_routes_f19_headways.csv', row.names=FALSE)
```

## Calculate the number of bus / CR routes and riders that connect to each station

```{r}
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

### Bus ridership pre-processing

```{r}
bus_ridership_f23 <- read.csv('data/mbta/bus_ridership_stops_F23.csv')
bus_ridership_f19 <- read.csv('data/mbta/bus_ridership_stops_F19.csv')
```

```{r}
clean_bus_ridership <- function(bus_df) {
  bus_df %>% 
    mutate(route_id = as.character(route_id)) %>%
    group_by(route_id) %>%
    summarise(across(c(average_ons, average_offs), sum), .groups='drop')
}

bus_rdrs_f23 <- bus_ridership_f23 %>% clean_bus_ridership()
bus_rdrs_f19 <- bus_ridership_f19 %>% clean_bus_ridership()
```

```{r}
calc_bus_metrics <- function(gtfs, bus_ridership) {
  # the goal here is to count the number of bus routes and riders connecting to each parent_station
  gtfs$stops %>% 
    inner_join(gtfs$stop_times, by='stop_id') %>%
    inner_join(gtfs$trips, by='trip_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    # the idea to filter by listed_route != 1 comes from the MBTA GTFS documentation:
    # https://github.com/mbta/gtfs-documentation/blob/master/reference/gtfs.md#routestxt
    # in practice, it filters out a handful of variants that aren't really their own routes
    filter(str_detect(route_desc, 'Bus'), listed_route != '1') %>%
    select(parent_station, route_id) %>%
    unique() %>%
    # left join because not all bus route IDs have ridership in this dataset
    # I did check and it was just a handful of variants / special service type routes
    left_join(bus_ridership, by='route_id') %>%
    group_by(parent_station) %>%
    summarise(bus_rte_count = n(), bus_rte_riders = sum(average_ons, na.rm=T), .groups='drop')
}

f23_buses <- f23_gtfs %>% calc_bus_metrics(bus_rdrs_f23)
f19_buses <- f19_gtfs %>% calc_bus_metrics(bus_rdrs_f19)
```

```{r}
calc_cr_metrics <- function(gtfs) {
  gtfs$stops %>% 
    inner_join(gtfs$stop_times, by='stop_id') %>%
    inner_join(gtfs$trips, by='trip_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    # the idea to filter by listed_route != 1 comes from the MBTA GTFS documentation:
    # https://github.com/mbta/gtfs-documentation/blob/master/reference/gtfs.md#routestxt
    # in practice, it filters out a handful of variants that aren't really their own routes
    filter(route_desc == 'Commuter Rail', listed_route != '1') %>%
    select(parent_station, route_id) %>%
    unique() %>%
    group_by(parent_station) %>%
    summarise(cr_rte_count = n(), .groups='drop')
}

f23_cr <- f23_gtfs %>% calc_cr_metrics()
f19_cr <- f19_gtfs %>% calc_cr_metrics()
```

```{r}
clean_connections <- function(rt_stations_df, buses_df, cr_df) {
  rt_stations_df %>%
    left_join(buses_df, by='parent_station') %>%
    left_join(cr_df, by='parent_station') %>%
    mutate(
      connecting_cr_routes = case_when(
        is.na(cr_rte_count) ~ 0, .default=cr_rte_count
      ),
      connecting_bus_routes = case_when(
        is.na(bus_rte_count) ~ 0, .default=bus_rte_count
      ),
      connecting_bus_riders = case_when(
        is.na(bus_rte_riders) ~ 0, .default=bus_rte_riders
      )) %>%
    select(route_id, parent_station, connecting_cr_routes, 
           connecting_bus_routes, connecting_bus_riders)
}

rt_stations_f23 %>% clean_connections(f23_buses, f23_cr) %>%
  write.csv('data/cleaned/stations_routes_f23_bus&CR_connections.csv', row.names=FALSE)

rt_stations_f19 %>% clean_connections(f19_buses, f19_cr) %>%
  write.csv('data/cleaned/stations_routes_f19_bus&CR_connections.csv', row.names=FALSE)
```

For travel times and headways, we can either use schedules (reflecting "promised" service) or actual service. For schedules, we just need the GTFS. For actuals, we need GTFS and the travel time and headway datasets described below.

# MBTA Rapid Transit Travel Times

This is a very large dataset that will require us to:
- filter dates to our specific season
- choose which destinations we should consider for each station origin (or if some kind of destination-weighted average travel time is possible / desirable)

Follow these instructions to run this part of the analysis:

1. Download the MBTA Rapid Transit Travel Times 2023 dataset from this link: https://mbta-massdot.opendata.arcgis.com/datasets/b08854b18ad942e4bc471c8c20e80792/about

2019 link: https://mbta-massdot.opendata.arcgis.com/datasets/a70a2151c74f4ea7b6688970b76d2ff7/about 

2. Run this next cell to interactively choose the folder where you downloaded the dataset:

```{r}
downloaded_traveltimes_path <- choose.dir(default = "", caption = "Select the travel times folder:")
```

Steps 3 and 4 are not necessary if you already initalized a downloaded_gtfs_path to process other GTFS-based metrics.

3. Download the MBTA GTFS recap dataset from this link: https://www.arcgis.com/home/item.html?id=9ab1dc7ea2bf4ad7b7e25cc6b941b39a

4. Run this next cell to interactively choose the folder where you downloaded the GTFS dataset:

```{r}
downloaded_gtfs_path <- choose.dir(default = "", caption = "Select the GTFS recaps folder:")
```

Now read in the data. I am not sure whether this path syntax will work on non-Windows machines.

```{r}
q3q4_2023_ttimes_rt <- 
  read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q3_HRTravelTimes.csv')) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q3_LRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q4_HRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q4_LRTravelTimes.csv')))


q3q4_2019_ttimes_rt <- 
  read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q3_HRTravelTimes.csv')) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q3_LRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q4_HRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q4_LRTravelTimes.csv')))


f23_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2023-fall-prerating-recap.zip'))
f19_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2019-fall-prerating-recap.zip'))

rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
travel_time_metrics <- function(travel_times, gtfs) {
  ttimes_filtered <- gtfs$calendar %>%
    inner_join(gtfs$trips, by='service_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    filter(route_desc == 'Rapid Transit') %>%
    summarise(
      rating_start = as.character(min(start_date)), 
      rating_end = as.character(max(end_date))
      ) %>%
    inner_join(travel_times, 
      by=join_by(rating_start <= service_date, rating_end >= service_date))
  
  rt_stops <- gtfs$routes %>%
    filter(route_desc == 'Rapid Transit') %>%
    inner_join(gtfs$trips, by='route_id') %>%
    inner_join(gtfs$stop_times, by='trip_id') %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    select(stop_id, parent_station, stop_name) %>%
    unique() %>%
    mutate(stop_id = as.integer(stop_id)) 
  
  rt_stops %>%
    filter(stop_name %in% c('Park Street', 'Downtown Crossing', 'Government Center', 'State')) %>%
    inner_join(ttimes_filtered, by=join_by(stop_id == to_stop_id)) %>%
    mutate(route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)) %>%
    group_by(route_id, from_stop_id, stop_id) %>%
    summarise(
      runs=n(), 
      avg_tt = mean(travel_time_sec, na.rm=T), 
      sd_tt = sd(travel_time_sec, na.rm=T),
      .groups='drop') %>%
    inner_join(rt_stops, by=join_by(from_stop_id == stop_id)) %>%
    group_by(route_id, parent_station) %>% 
    slice(which.min(avg_tt)) %>%
    select(route_id, parent_station, avg_tt, sd_tt)
} 

ttime_metrics_f23 <- travel_time_metrics(q3q4_2023_ttimes_rt, f23_gtfs)
ttime_metrics_f19 <- travel_time_metrics(q3q4_2019_ttimes_rt, f19_gtfs)
```

```{r}
# the only rows that should show up here are Mattapan Line stops, for which the MBTA
# does not have / does not publish travel time data
rt_stations_f23 %>%
  left_join(ttime_metrics_f23, by=c('route_id', 'parent_station')) %>%
  filter(is.na(avg_tt))
```

```{r}
ttime_metrics_f23 %>% 
  write.csv('data/cleaned/stations_routes_f23_travel_times_to_cbd.csv', row.names=FALSE)
ttime_metrics_f19 %>% 
  write.csv('data/cleaned/stations_routes_f19_travel_times_to_cbd.csv', row.names=FALSE)
```

# MBTA Rapid Transit Headways 

This is a very large dataset that will require us to:
- filter dates to our specific season
- calculate average or median headways per station (/direction?) for our specific season

2023 link: https://mbta-massdot.opendata.arcgis.com/datasets/896d043f33ae4915b530df12141c772c_0/explore
2019 link: https://mbta-massdot.opendata.arcgis.com/datasets/5c36245a6e7f4b89851b99a4a2bce02b_0/explore

# Walking-Distance Buffer Areas

This requires setting up an OpenRouteService API key (free) as described here: https://giscience.github.io/openrouteservice-r/articles/openrouteservice.html 

```{r}
library(openrouteservice)

rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

Be careful about how many times you run this - the openrouteservice isochrones API limits you to 500 requests per day per API key. Each request returns 5 polygons, and we have ~150 stops, so a full run will use ~30 requests. There's also 20 request per minute throttling, so 1 full run per dataframe will take ~1.5 minutes.
```{r}
walk_polys <- function(rt_stations_df) {
  rt_stations_df %>%
    mutate( # API expects a df with these two col names and nothing else
      lon=stop_lon, lat=stop_lat, .keep='none'
      ) %>% 
    split(rep(1:ceiling(nrow(rt_stations_df)/5),each=5)) %>% # split into groups of 5
    lapply(function(df) {
      Sys.sleep(3.01) # pause in between requests to accommodate the 20 per minute limit
      ors_isochrones(df, profile=ors_profile(mode='walking'), range=600, output='sf')
      }) %>%
    bind_rows()
} 

walk_polys_f23 <- walk_polys(rt_stations_f23)
walk_polys_f19 <- walk_polys(rt_stations_f19)
```

This plot shows that many of the 10min walking polygons overlap, even on the same route. The next step addresses this potential violation of the independence assumption by converting these into polygons that don't overlap each other on the same routes.
```{r}
library(sf)

plot(st_geometry(walk_polys_f23))
```

Source for the following function: https://gis.stackexchange.com/questions/358797/splitting-overlap-between-polygons-and-assign-to-nearest-polygon-using-r 
```{r}
st_no_overlap <- function(polygons) {

  centroids <- st_centroid(polygons)

  # Voronoi tesselation
  voronoi <-
    centroids %>%
    st_geometry() %>%
    st_union() %>%
    st_voronoi() %>%
    st_collection_extract()

  # Put them back in their original order
  voronoi <- voronoi[unlist(st_intersects(centroids, voronoi))]

  # Keep the attributes
  result <- centroids

  # Intersect voronoi zones with polygons
  st_geometry(result) <-
    mapply(function(x, y) st_intersection(x, y),
           st_geometry(polygons),
           voronoi,
           SIMPLIFY = FALSE) %>%
    st_sfc(crs = st_crs(polygons))

  result
}
```

```{r}
walk_polys_f23[c('route_id', 'parent_station')] <- rt_stations_f23[c('route_id', 'parent_station')]
walk_polys_f19[c('route_id', 'parent_station')] <- rt_stations_f19[c('route_id', 'parent_station')]
```

```{r}
sf_use_s2(FALSE)

walk_polys_nonoverlap <- function(walk_polys_df) {
  walk_polys_df %>%
  st_transform(26986) %>% # convert to projected coord system before no_overlap calc
  group_split(route_id) %>% # split by route ID to limit the nonoverlap calculation to route-level
  lapply(function(df) st_no_overlap(df)) %>% 
  bind_rows() %>% # re-combine the list of dataframes by route into a single df
  st_transform(4326) # convert back to lat/long
}

walk_polys_f23_nonoverlap <- walk_polys_nonoverlap(walk_polys_f23)
walk_polys_f19_nonoverlap <- walk_polys_nonoverlap(walk_polys_f19)
```

Now, the plot shows the walk areas as nonoverlapping at the route level.
```{r}
plot(st_geometry(walk_polys_f23_nonoverlap))
```

Save the station areas with both an overlapping version and a non-overlapping version. 
```{r}
clean_walk_polys <- function(walk_polys_df) {
  walk_polys_df %>%
    mutate(walk_poly_wkt = st_as_text(geometry)) %>%
    st_drop_geometry() %>%
    select(route_id, parent_station, walk_poly_wkt)
}

walk_polys_f23 %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f23_geoms_overlapping.csv', row.names=FALSE)

walk_polys_f23_nonoverlap %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f23_geoms_nonoverlapping.csv', row.names=FALSE)

walk_polys_f19 %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f19_geoms_overlapping.csv', row.names=FALSE)

walk_polys_f19_nonoverlap %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f19_geoms_nonoverlapping.csv', row.names=FALSE)
```

## CTPS park-and-ride inventory for MBTA stations

Downloaded from https://www.ctps.org/maploc/www/apps/pnr-dashboard/index.html and stored in the data folder.

```{r}
ctps <- read.csv('data/mbta/ctps_pnr_lots_polygons.csv')
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
ctps_clean <- ctps %>%
  filter(mode == 'Rapid Transit') %>%
  mutate(station = str_trim(station, side='both')) %>%
  mutate(station = case_when(station == 'Malden' ~ 'Malden Center', .default=station)) %>%
  # total spaces can be disaggregated by permit-only, handicap-only, and general public spots
  select(station, station_name, line_id, total_spaces_1) %>%
  group_by(station) %>%
  summarise(spaces = sum(total_spaces_1), .groups='drop')
```

```{r}
# used this join to figure out which station names needed to be recoded - this
# should have 0 rows. Any rows represent station names that didn't match successfully
ctps_clean %>%
  left_join(rt_stations_f23, by=join_by(station == stop_name)) %>%
  filter(is.na(route_id))
```

```{r}
join_ctps <- function(rt_stations_df) {
  rt_stations_df %>%
  left_join(ctps_clean, by=join_by(stop_name == station)) %>%
  mutate(pnr_spaces = case_when(is.na(spaces) ~ 0, .default=spaces), .keep='unused') %>%
  select(route_id, parent_station, pnr_spaces)
}

rt_stations_f23 %>% join_ctps() %>% write.csv('data/cleaned/stations_routes_f23_pnr_spaces.csv', row.names=FALSE)
rt_stations_f19 %>% join_ctps() %>% write.csv('data/cleaned/stations_routes_f19_pnr_spaces.csv', row.names=FALSE)
```

# Walk Scores

```{r}
ws <- read.csv('data/rapid_trans_facilities_wlkscore.csv')
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
# these selections should have 0 rows. Any rows represent stations that didn't join to the walk scores
rt_stations_f23 %>%
  left_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  filter(is.na(walk_score))

rt_stations_f19 %>% select(-stop_name) %>%
  left_join(rt_stations_f23 %>% select(stop_name, route_id, parent_station), 
             by=c('route_id', 'parent_station')) %>%
  left_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  filter(is.na(walk_score))
```

The three missing-score stops from 2019 are the stops that were removed in the B branch consolidation project. Could be nice to get walk scores for those, but they can be removed for now

```{r}
rt_stations_f23 %>%
  inner_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  select(route_id, parent_station, walk_score) %>%
  write.csv('data/cleaned/stations_routes_f23_walk_scores.csv', row.names=FALSE)

rt_stations_f19 %>% select(-stop_name) %>%
  inner_join(rt_stations_f23 %>% select(stop_name, route_id, parent_station), 
             by=c('route_id', 'parent_station')) %>%
  inner_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  select(route_id, parent_station, walk_score) %>%
  write.csv('data/cleaned/stations_routes_f19_walk_scores.csv', row.names=FALSE)
```

# Geography

```{r}
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
library(sf)

bos <- st_read('data/boston_boundary/City_of_Boston_Boundary_(Water_Included).shp')
```

```{r}
calc_geog_metrics <- function(rt_stations_df) {
  station_points <- rt_stations_df %>%
    st_as_sf(coords = c("stop_lon","stop_lat"), crs=4326)
  
  station_points$in_bos <- station_points %>%
    st_intersects(bos %>% st_transform(4326)) %>%
    as.logical() %>% replace_na(0) %>% as.numeric()
  
  cbd <- st_sfc(st_point(c(-71.061418, 42.354919))) %>% 
    st_set_crs(4326)
  
  station_points$km_from_cbd <- station_points %>%
    st_distance(cbd) %>% as.numeric() / 1000
  
  station_points %>%
    st_drop_geometry() %>%
    select(-stop_name)
}

rt_stations_f23 %>% calc_geog_metrics() %>% 
  write.csv('data/cleaned/stations_routes_f23_geog_vars.csv', row.names=FALSE)
rt_stations_f19 %>% calc_geog_metrics() %>% 
  write.csv('data/cleaned/stations_routes_f19_geog_vars.csv', row.names=FALSE)
```

# College Locations & Enrollments (IPEDS)

2023 and 2019 data download links and data dictionaries available from https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx?gotoReportId=7&fromIpeds=true 

```{r}
# directory information - contains the names and coords of educational institutions
hd_23 <- read.csv('data/ipeds/hd2023.csv')
hd_19 <- read.csv('data/ipeds/hd2019.csv')

# 12-month enrollment per educational institution
effy_23 <- read.csv('data/ipeds/effy2023.csv')
effy_19 <- read.csv('data/ipeds/effy2019.csv')

# station-area geometries to which we will join the institution coordinates
rt_station_geoms_f23 <- read.csv('data/cleaned/stations_routes_f23_geoms_nonoverlapping.csv')
rt_station_geoms_f19 <- read.csv('data/cleaned/stations_routes_f19_geoms_nonoverlapping.csv')
```

```{r}
library(sf) 

enrollment_23 <- effy_23 %>%
  filter(EFFYALEV == 1) %>% # this code is for the "All students total" slice
  select(UNITID, EFYTOTLT) # this variable is the actual enrollment number

# annoyingly, the variable name changed slightly from EFFYLEV to EFFYALEV
enrollment_19 <- effy_23 %>%
  filter(EFFYLEV == 1) %>% # this code is for the "All students total" slice
  select(UNITID, EFYTOTLT) # this variable is the actual enrollment number

ma_college_data <- function(hd, enrollment) {
  hd %>%
    filter(STABBR == 'MA') %>%
    select(UNITID, INSTNM, LONGITUD, LATITUDE) %>%
    inner_join(enrollment, by='UNITID') %>%
    st_as_sf(crs=4326, coords = c("LONGITUD","LATITUDE")) %>%
    st_transform(crs=26986) # convert from lat-long to projected coord system for MA
}

ma_colleges_23 <- ma_college_data(hd_23, enrollment_23)
ma_colleges_19 <- ma_college_data(hd_19, enrollment_19)
```

```{r}
join_college_data <- function(rt_station_geoms, ma_college_df) {
  rt_station_geoms_f23 %>%
    st_as_sf(crs=4326, wkt = "walk_poly_wkt") %>%
    st_transform(crs=26986) %>%
    st_join(ma_colleges_23, join=st_intersects, left=TRUE) %>%
    
    # splits point enrollments evenly across intersecting station areas - unsure whether to use this
    group_by(UNITID) %>%
    mutate(EFYTOTLT = EFYTOTLT/n()) %>%
    ungroup() %>%
  
    group_by(route_id, parent_station) %>%
    summarise(college_students = sum(EFYTOTLT, na.rm=T), .groups='drop') %>%
    st_drop_geometry()
}

rt_station_geoms_f23 %>% 
  join_college_data(ma_colleges_23) %>%
  write.csv('data/cleaned/stations_routes_f23_colleges.csv', row.names=FALSE)

rt_station_geoms_f19 %>% 
  join_college_data(ma_colleges_19) %>%
  write.csv('data/cleaned/stations_routes_f19_colleges.csv', row.names=FALSE)
```

