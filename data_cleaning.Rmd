---
title: "Data Cleaning"
author: "Emmett Greenberg, Ted Banken, Ethan McIntosh"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytransit)
```

This script will house any data cleaning processes to go from raw data to model-ready intermediate outputs. These processes are in their own script file so that we don't want to do them every time we build a model.

## MBTA ridership data (dependent variable)

```{r}
# these data come from the MBTA open data portal. Link for humans: https://mbta-massdot.opendata.arcgis.com/search?q=rail%20ridership%20by%20time%20period 

ridership_f23 <- read.csv('https://hub.arcgis.com/api/v3/datasets/80a379ebaa374cfd836ca4d3880ceda4_0/downloads/data?format=csv&spatialRefId=4326&where=1%3D1')

ridership_f17_to_f19 <- read.csv('data/mbta/rail_ridership_stops_F17toF19.csv')
```

```{r}
riders_period <- ridership_f23 %>% 
  mutate(period = case_when(
    time_period_name %in% c('AM_PEAK', 'PM_PEAK') ~ str_to_lower(time_period_name)
    , .default='off_peak'
    )) %>%
  group_by(season, day_type_id, period, route_id, parent_station) %>%
  summarise( # first, sum up across the time periods within weekday off-peak
    across(c(total_ons, total_offs), sum), 
    num_svc_days = mean(number_service_days), # avg bc svc days are equal within day types
    .groups='drop') 

riders_day_type <- riders_period %>%
  group_by(season, day_type_id, route_id, parent_station) %>%
  summarise( # sum up across the time periods within day types
    across(c(total_ons, total_offs), sum), 
    num_svc_days = mean(num_svc_days), # avg bc svc days are equal within day types
    .groups='drop') %>% 
  mutate( # create a binary daytype of just weekday vs weekend
    daytype = case_when(day_type_id == 'day_type_01' ~ 'wkdy', .default='wknd'),
    ) 

riders_fullweek <- riders_day_type %>% 
  group_by(season, route_id, parent_station) %>%
  summarise( # sum up the boardings for the entire week regardless of day type
    across(c(total_ons, total_offs, num_svc_days), sum), 
    .groups='drop') %>%
  mutate(
    avg_boardings = total_ons/num_svc_days,
    avg_alightings = total_offs/num_svc_days,
    .keep='unused'
    )

riders_wkdy_periods <- riders_period %>%
  filter(day_type_id == 'day_type_01') %>%
  select(-day_type_id) %>%
  mutate(
    avg_boardings = total_ons/num_svc_days,
    avg_alightings = total_offs/num_svc_days,
    .keep='unused'
    ) %>%
  pivot_wider( # create separate columns for wkdy vs wknd boardings
    id_cols = c("season", "route_id", "parent_station"),
    names_from = 'period', values_from = c('avg_boardings', 'avg_alightings')
  )

riders_day_type %>%
  group_by(season, daytype, route_id, parent_station) %>%
  summarise( # sum up to the daytype (weekday vs weekend) level
    across(c(total_ons, total_offs, num_svc_days), sum), 
    .groups='drop') %>%
  mutate(
    avg_boardings = total_ons/num_svc_days,
    avg_alightings = total_offs/num_svc_days,
    .keep='unused'
    ) %>%
  pivot_wider( # create separate columns for wkdy vs wknd boardings
    id_cols = c("season", "route_id", "parent_station"),
    names_from = 'daytype', values_from = c('avg_boardings', 'avg_alightings')
  ) %>% # then join with the weekday period-specific boardings
  inner_join(riders_wkdy_periods, by=c("season", "route_id", "parent_station")) %>% 
  # and join with the more general avg_boardings for the full week
  inner_join(riders_fullweek, by=c("season", "route_id", "parent_station"))
```

```{r}
clean_ridership <- function(ridership) {
  riders_period <- ridership %>% 
    mutate(period = case_when(
      time_period_name %in% c('AM_PEAK', 'PM_PEAK') ~ str_to_lower(time_period_name)
      , .default='off_peak'
      )) %>%
    group_by(season, day_type_id, period, route_id, parent_station) %>%
    summarise( # first, sum up across the time periods within weekday off-peak
      across(c(total_ons, total_offs), sum), 
      num_svc_days = mean(number_service_days), # avg bc svc days are equal within day types
      .groups='drop') 
  
  riders_day_type <- riders_period %>%
    group_by(season, day_type_id, route_id, parent_station) %>%
    summarise( # sum up across the time periods within day types
      across(c(total_ons, total_offs), sum), 
      num_svc_days = mean(num_svc_days), # avg bc svc days are equal within day types
      .groups='drop') %>% 
    mutate( # create a binary daytype of just weekday vs weekend
      daytype = case_when(day_type_id == 'day_type_01' ~ 'wkdy', .default='wknd'),
      ) 
  
  riders_fullweek <- riders_day_type %>% 
    group_by(season, route_id, parent_station) %>%
    summarise( # sum up the boardings for the entire week regardless of day type
      across(c(total_ons, total_offs, num_svc_days), sum), 
      .groups='drop') %>%
    mutate(
      avg_boardings = total_ons/num_svc_days,
      avg_alightings = total_offs/num_svc_days,
      .keep='unused'
      )
  
  riders_wkdy_periods <- riders_period %>%
    filter(day_type_id == 'day_type_01') %>%
    select(-day_type_id) %>%
    mutate(
      avg_boardings = total_ons/num_svc_days,
      avg_alightings = total_offs/num_svc_days,
      .keep='unused'
      ) %>%
    pivot_wider( # create separate columns for wkdy vs wknd boardings
      id_cols = c("season", "route_id", "parent_station"),
      names_from = 'period', values_from = c('avg_boardings', 'avg_alightings')
    )
  
  riders_day_type %>%
    group_by(season, daytype, route_id, parent_station) %>%
    summarise( # sum up to the daytype (weekday vs weekend) level
      across(c(total_ons, total_offs, num_svc_days), sum), 
      .groups='drop') %>%
    mutate(
      avg_boardings = total_ons/num_svc_days,
      avg_alightings = total_offs/num_svc_days,
      .keep='unused'
      ) %>%
    pivot_wider( # create separate columns for wkdy vs wknd boardings
      id_cols = c("season", "route_id", "parent_station"),
      names_from = 'daytype', values_from = c('avg_boardings', 'avg_alightings')
    ) %>% # then join with the weekday period-specific boardings
    inner_join(riders_wkdy_periods, by=c("season", "route_id", "parent_station")) %>% 
    # and join with the more general avg_boardings for the full week
    inner_join(riders_fullweek, by=c("season", "route_id", "parent_station"))
}

f23_clean <- ridership_f23 %>% clean_ridership()
f19_clean <- ridership_f17_to_f19 %>%
  filter(season=='Fall 2019') %>%
  mutate(parent_station = stop_id) %>% 
  clean_ridership()

f19_clean %>% write.csv('data/cleaned/stations_routes_f19_ridership.csv', row.names=F)
f23_clean %>% write.csv('data/cleaned/stations_routes_f23_ridership.csv', row.names=F)
```

The unique identifier per ridership row is the combination of route_id and parent_station (meaning, for example, that State Street has separate rows for the Orange Line and the Blue Line). This informs how we process the various independent variables.

# MBTA GTFS

GTFS is a good canonical source for the lat/long coordinates for each station. GTFS is also probably the best source for calculating the number (and names) of bus routes that serve a given RT station. 

It is also an option to calculate scheduled travel times and headways from GTFS, as an alternative to using data on actual headways and travel times.

GTFS feeds have files larger than the GitHub size limit, so they're not included in the repo. Follow these instructions to run this part of the analysis:

1. Download the MBTA GTFS recap dataset from this link: https://www.arcgis.com/home/item.html?id=9ab1dc7ea2bf4ad7b7e25cc6b941b39a

2. Run this next cell to interactively choose the folder where you downloaded the dataset:

```{r}
downloaded_gtfs_path <- choose.dir(default = "", caption = "Select the GTFS recaps folder:")
```

I am not sure whether this path syntax will work on non-Windows machines.

```{r}
f23_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2023-fall-prerating-recap.zip'))
f19_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2019-fall-prerating-recap.zip'))
```

## select stops from stop_times on trips belonging to Rapid Transit routes

```{r}
rt_stations_per_route <- function(gtfs) { 
  gtfs$routes %>%
    filter(route_desc == 'Rapid Transit') %>% # filter to Rapid Transit routes
    inner_join(gtfs$trips, by='route_id') %>%
    inner_join(gtfs$stop_times, by='trip_id') %>% # get all stop times on these routes
    inner_join(gtfs$stops, by='stop_id') %>%
    # reduce the Green Line's branches into a single Green route_id, consistent with ridership
    mutate(
      route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)
      ) %>%
    select(route_id, route_long_name, parent_station) %>%
    # this second join to the stops table is so that we use the parent station IDs and their more
    # generalized coordinates instead of the more specific coordinates of each platform
    inner_join(gtfs$stops, by=join_by(parent_station==stop_id)) %>%
    select(route_id, parent_station, stop_name, stop_lat, stop_lon) %>%
    unique() # this selection lists each stop once per route they serve
}

rt_stations_f23 <- rt_stations_per_route(f23_gtfs)
rt_stations_f19 <- rt_stations_per_route(f19_gtfs)
rt_stations_f23
```
Save the station-route level location and name information to file

```{r}
rt_stations_f23 %>% write.csv('data/cleaned/stations_routes_f23.csv', row.names=FALSE)
rt_stations_f19 %>% write.csv('data/cleaned/stations_routes_f19.csv', row.names=FALSE)
```

## Headways and Terminal Stops

Helper function used for both calculations to identify typical Rapid Transit service
```{r}
typical_rt_service <- function(gtfs) {
  gtfs$stop_times %>% 
    inner_join(gtfs$trips, by='trip_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    filter(route_desc == 'Rapid Transit') %>%
    inner_join(gtfs$calendar_attributes, by='service_id') %>%
    filter(service_schedule_typicality == 1) %>% # filter out atypical (e.g. special event) services
    mutate(direction_id = as.character(direction_id)) %>% # data type fix to avoid errors for next join
    inner_join(gtfs$route_patterns, by=c('route_pattern_id', 'route_id', 'direction_id')) %>%
    filter(route_pattern_typicality == 1) # filter out atypical (e.g. early morning) variants
}
```

### Terminals

```{r}
calc_terminals <- function(gtfs) {
  gtfs %>% 
    typical_rt_service() %>% # for each route, branch (route pattern) and direction... 
    group_by(route_id, route_pattern_id, direction_id) %>% 
    slice(which.min(stop_sequence)) %>% # ...find the origin (first) stops of each trip
    ungroup() %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    mutate( # reduce the GL's branches into a single Green route_id, consistent with ridership
      route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)
      ) %>%
    select(route_id, parent_station) %>%
    unique() %>%
    mutate(terminal=1) # assign these stations the value 1 for the terminal dummy variable
}

f23_terminals <- f23_gtfs %>% calc_terminals()
f19_terminals <- f19_gtfs %>% calc_terminals()

f23_terminals
```

```{r}
f23_terminals %>% write.csv('data/cleaned/stations_routes_f23_terminals.csv', row.names=FALSE)
f19_terminals %>%   write.csv('data/cleaned/stations_routes_f19_terminals.csv', row.names=FALSE)
```

### Headways (average inbound, in seconds) per station, route, daytype

```{r}
library(hms)

headways_per_station_route <- function(gtfs) { 
  gtfs %>% 
    typical_service() %>%
    filter( # only consider core service hours and a single direction (inbound, in this case)
      departure_time > parse_hms('06:00:00'), 
      departure_time < parse_hms('22:00:00'),
      direction_id == '1') %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    mutate( # reduce the GL's branches into a single Green route_id, consistent with ridership
      route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)
      ) %>% # for each station, route, and day type (weekday / Sat / Sun)...
    group_by(route_id, parent_station, service_schedule_type) %>%
    # calculate avg headway as total time elapsed / total # of departures during the period
    summarise(
      avg_headway = round(16*3600/n()), service_count = n_distinct(service_id),
      .groups='drop') 
}

f23_headways <- f23_gtfs %>% headways_per_station_route()
f19_headways <- f19_gtfs %>% headways_per_station_route()
```

```{r}
# this diagnostic checks that all the headways were calculated with a single service ID
# if there are values other than 1 in this output, the headways may need recalculation
f23_headways %>%
  select(service_schedule_type, service_count) %>%
  unique()

f19_headways %>%
  select(service_schedule_type, service_count) %>%
  unique()
```

Pivot the headways per daytype into their own columns and combine Sat & Sun into a general weekend
```{r}
pivot_daytypes <- function(headways) {
  headways %>%
    mutate( # create the binary daytype (weekday vs weekend) variable
      daytype = case_when(service_schedule_type == 'Weekday' ~ 'wkdy', .default='wknd')
      ) %>%
    group_by(route_id, parent_station, daytype) %>% # average the Sat & Sun headways
    summarise(avg_headway = mean(avg_headway), .groups='drop') %>%
    pivot_wider( # put wkdy vs wknd headways into their own columns
      id_cols = c('route_id', 'parent_station'), 
      names_from='daytype', 
      names_prefix = 'avg_headway_', 
      values_from='avg_headway'
      )
}

f23_headways %>% pivot_daytypes()
```

```{r}
f23_headways %>% pivot_daytypes() %>% 
  write.csv('data/cleaned/stations_routes_f23_headways.csv', row.names=FALSE)
f19_headways %>% pivot_daytypes() %>% 
  write.csv('data/cleaned/stations_routes_f19_headways.csv', row.names=FALSE)
```

## Interstation spacing

```{r}
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

This may take several minutes to run because the geographic functions take a little while to run row-wise. There probably is a way to refactor this and speed it up if that becomes a priority.
```{r}
calculate_spacing <- function(gtfs, rt_stations_df) {
  ordered_stops <- gtfs %>%
    typical_rt_service() %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    select(direction_id, route_id, route_pattern_id, parent_station, stop_sequence) %>%
    unique() %>%
    filter(direction_id == 1) %>%
    group_by(route_id, route_pattern_id) %>%
    # rank is the numbered order of the stops on each route & branch (route pattern)
    mutate(rank = with_order(order_by = stop_sequence,
                             fun      = row_number, 
                             x        = desc(stop_sequence)
                             ),
           rank_next = rank + 1,
           rank_prev = rank - 1
           ) %>%
    select(-c(direction_id, stop_sequence)) %>%
    ungroup() %>%
    mutate( # reduce the GL's branches into a single Green route_id, consistent with ridership
      route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)
      )
  
  ordered_stops %>% # join each stop to it's next stop per route
    left_join(ordered_stops, suffix=c('', '_next'), by=join_by(
      route_id == route_id, route_pattern_id == route_pattern_id, rank == rank_next
      )) %>% # and its previous stop per route
    left_join(ordered_stops, suffix=c('', '_prev'), by=join_by(
      route_id == route_id, route_pattern_id == route_pattern_id, rank == rank_prev
      )) %>%
    select(route_id, parent_station, parent_station_prev, parent_station_next) %>%
    left_join( # and then join in the coordinates for each stop,
      rt_stations_df %>% select(-stop_name), 
      by=join_by(route_id == route_id, parent_station == parent_station)
      ) %>%
    left_join( # join in the coordinates for each previous stop, attaching _prev suffix
      rt_stations_df %>% select(-stop_name), suffix = c('', '_prev'),
      by=join_by(route_id == route_id, parent_station_prev == parent_station)
      ) %>%
    left_join( # join in the coordinates for each previous stop, attaching _next suffix,
      rt_stations_df %>% select(-stop_name), suffix = c('', '_next'),
      by=join_by(route_id == route_id, parent_station_next == parent_station)
      ) %>%
    rowwise() %>%
    mutate( # and then calculate the distance in km between each stop...
      dist_prev_km = st_distance(
        st_point(c(stop_lon, stop_lat)) %>% 
          st_geometry() %>% st_set_crs(4326) %>% st_transform(26986), 
        st_point(c(stop_lon_prev, stop_lat_prev)) %>% # ...and its previous stop...
          st_geometry() %>% st_set_crs(4326) %>% st_transform(26986)
        ) %>% as.numeric() / 1000,
      dist_next_km = st_distance(
        st_point(c(stop_lon, stop_lat)) %>% 
          st_geometry() %>% st_set_crs(4326) %>% st_transform(26986), 
        st_point(c(stop_lon_next, stop_lat_next)) %>% # ...and its next stop...
          st_geometry() %>% st_set_crs(4326) %>% st_transform(26986)
        ) %>% as.numeric() / 1000,
      # ...then average them, removing NAs (eg terminal stops with no next stop)
      avg_spacing_km = mean(c(dist_prev_km, dist_next_km), na.rm=T) 
    ) %>%
    group_by(route_id, parent_station) %>%
    # this final step averages the spacings across branches (e.g. for JFK)
    summarise(avg_spacing_km = mean(avg_spacing_km), .groups='drop')
}

f23_spacings <- f23_gtfs %>% calculate_spacing(rt_stations_f23)
f19_spacings <- f19_gtfs %>% calculate_spacing(rt_stations_f19)
```

```{r}
f23_spacings %>% write.csv('data/cleaned/stations_routes_f23_spacings.csv', row.names=FALSE)
f19_spacings %>% write.csv('data/cleaned/stations_routes_f19_spacings.csv', row.names=FALSE)
```

## Bus / CR routes and riders that connect to each station

```{r}
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

Helper function for both calculations to get a list of station-routes served by a given mode
```{r}
stations_per_mode <- function(gtfs, mode) {
  gtfs$stops %>% 
    inner_join(gtfs$stop_times, by='stop_id') %>%
    inner_join(gtfs$trips, by='trip_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    # the idea to filter by listed_route != 1 comes from the MBTA GTFS documentation:
    # https://github.com/mbta/gtfs-documentation/blob/master/reference/gtfs.md#routestxt
    # in practice, it filters out a handful of variants that aren't really their own routes
    filter(str_detect(route_desc, mode), listed_route != '1') %>%
    select(parent_station, route_id) %>%
    unique()
}
```

### Bus 

ridership pre-processing
```{r}
bus_ridership_f23 <- read.csv('data/mbta/bus_ridership_stops_F23.csv')
bus_ridership_f19 <- read.csv('data/mbta/bus_ridership_stops_F19.csv')
```

```{r}
clean_bus_ridership <- function(bus_df) {
  bus_df %>% 
    mutate(route_id = as.character(route_id)) %>%
    group_by(route_id) %>%
    summarise(across(c(average_ons, average_offs), sum), .groups='drop')
}

bus_rdrs_f23 <- bus_ridership_f23 %>% clean_bus_ridership()
bus_rdrs_f19 <- bus_ridership_f19 %>% clean_bus_ridership()
```

```{r}
calc_bus_metrics <- function(gtfs, bus_ridership) {
  # count the number of bus routes and riders connecting to each parent_station
  gtfs %>% 
    stations_per_mode('Bus') %>% # get the list of station-routes serving Bus
    # left join because not all bus route IDs have ridership in this dataset
    # I did check and it was just a handful of variants / special service type routes
    left_join(bus_ridership, by='route_id') %>%
    group_by(parent_station) %>% # count routes and riders serving each parent station
    summarise(bus_rte_count = n(), bus_rte_riders = sum(average_ons, na.rm=T), .groups='drop')
}

f23_buses <- f23_gtfs %>% calc_bus_metrics(bus_rdrs_f23)
f19_buses <- f19_gtfs %>% calc_bus_metrics(bus_rdrs_f19)
```

### Commuter Rail

```{r}
calc_cr_metrics <- function(gtfs) {
  gtfs %>% 
    stations_per_mode('Commuter Rail') %>%
    group_by(parent_station) %>%
    summarise(cr_rte_count = n(), .groups='drop')
}

f23_cr <- f23_gtfs %>% calc_cr_metrics()
f19_cr <- f19_gtfs %>% calc_cr_metrics()
```

```{r}
clean_connections <- function(rt_stations_df, buses_df, cr_df) {
  rt_stations_df %>%
    left_join(buses_df, by='parent_station') %>%
    left_join(cr_df, by='parent_station') %>%
    mutate(
      connecting_cr_routes = case_when(
        is.na(cr_rte_count) ~ 0, .default=cr_rte_count
      ),
      connecting_bus_routes = case_when(
        is.na(bus_rte_count) ~ 0, .default=bus_rte_count
      ),
      connecting_bus_riders = case_when(
        is.na(bus_rte_riders) ~ 0, .default=bus_rte_riders
      )) %>%
    select(route_id, parent_station, connecting_cr_routes, 
           connecting_bus_routes, connecting_bus_riders)
}

rt_stations_f23 %>% clean_connections(f23_buses, f23_cr) %>%
  write.csv('data/cleaned/stations_routes_f23_bus&CR_connections.csv', row.names=FALSE)

rt_stations_f19 %>% clean_connections(f19_buses, f19_cr) %>%
  write.csv('data/cleaned/stations_routes_f19_bus&CR_connections.csv', row.names=FALSE)
```

# MBTA Rapid Transit Travel Times

Follow these instructions to run this part of the analysis:

1. Download the MBTA Rapid Transit Travel Times 2023 dataset from this link: https://mbta-massdot.opendata.arcgis.com/datasets/b08854b18ad942e4bc471c8c20e80792/about

2019 link: https://mbta-massdot.opendata.arcgis.com/datasets/a70a2151c74f4ea7b6688970b76d2ff7/about 

2. Run this next cell to interactively choose the folder where you downloaded the dataset:

```{r}
downloaded_traveltimes_path <- choose.dir(default = "", caption = "Select the travel times folder:")
```

Steps 3 and 4 are not necessary if you already initalized a downloaded_gtfs_path to process other GTFS-based metrics.

3. Download the MBTA GTFS recap dataset from this link: https://www.arcgis.com/home/item.html?id=9ab1dc7ea2bf4ad7b7e25cc6b941b39a

4. Run this next cell to interactively choose the folder where you downloaded the GTFS dataset:

```{r}
downloaded_gtfs_path <- choose.dir(default = "", caption = "Select the GTFS recaps folder:")
```

Now read in the data. I am not sure whether this path syntax will work on non-Windows machines.

```{r}
q3q4_2023_ttimes_rt <- 
  read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q3_HRTravelTimes.csv')) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q3_LRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q4_HRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2023-Q4_LRTravelTimes.csv')))


q3q4_2019_ttimes_rt <- 
  read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q3_HRTravelTimes.csv')) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q3_LRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q4_HRTravelTimes.csv'))) %>%
  rbind(read.csv(paste0(downloaded_traveltimes_path, '\\2019-Q4_LRTravelTimes.csv')))


f23_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2023-fall-prerating-recap.zip'))
f19_gtfs <- read_gtfs(paste0(downloaded_gtfs_path, '\\2019-fall-prerating-recap.zip'))

rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

Calculate travel times to the central business district (CBD)
```{r}
travel_time_metrics <- function(travel_times, gtfs) {
  ttimes_filtered <- gtfs$calendar %>%
    inner_join(gtfs$trips, by='service_id') %>%
    inner_join(gtfs$routes, by='route_id') %>%
    filter(route_desc == 'Rapid Transit') %>%
    summarise(
      rating_start = as.character(min(start_date)), 
      rating_end = as.character(max(end_date))
      ) %>%
    inner_join(travel_times, # filter travel times to the dates of the GTFS schedule
      by=join_by(rating_start <= service_date, rating_end >= service_date))
  
  rt_stops <- gtfs$routes %>% # grab the list of Rapid Transit stops for reference
    filter(route_desc == 'Rapid Transit') %>%
    inner_join(gtfs$trips, by='route_id') %>%
    inner_join(gtfs$stop_times, by='trip_id') %>%
    inner_join(gtfs$stops, by='stop_id') %>%
    select(stop_id, parent_station, stop_name) %>%
    unique() %>%
    mutate(stop_id = as.integer(stop_id)) 
  
  rt_stops %>% # filter to travel times for trips ending at the 4 downtown transfer stations
    filter(stop_name %in% c('Park Street', 'Downtown Crossing', 'Government Center', 'State')) %>%
    inner_join(ttimes_filtered, by=join_by(stop_id == to_stop_id)) %>%
    mutate( # reduce the GL's branches into a single Green route_id, consistent with ridership
      route_id = case_when(str_detect(route_id, 'Green') ~ 'Green', .default=route_id)
      ) %>%
    group_by(route_id, from_stop_id, stop_id) %>% # for each route, origin stop, and CBD stop
    summarise( # calculate the avg & sd of the travel time from that origin to that CBD stop
      avg_tt = mean(travel_time_sec, na.rm=T), 
      sd_tt = sd(travel_time_sec, na.rm=T),
      .groups='drop') %>%
    inner_join(rt_stops, by=join_by(from_stop_id == stop_id)) %>%
    group_by(route_id, parent_station) %>% 
    slice(which.min(avg_tt)) %>% # then take the minimum per origin (among the 4 CBD stops)
    select(route_id, parent_station, avg_tt, sd_tt) # to use as that stop's tt to CBD
} 

ttime_metrics_f23 <- travel_time_metrics(q3q4_2023_ttimes_rt, f23_gtfs)
ttime_metrics_f19 <- travel_time_metrics(q3q4_2019_ttimes_rt, f19_gtfs)
```

```{r}
# the only rows that should show up here are Mattapan Line stops, for which the MBTA
# does not have / does not publish travel time data
rt_stations_f23 %>%
  left_join(ttime_metrics_f23, by=c('route_id', 'parent_station')) %>%
  filter(is.na(avg_tt))
```

```{r}
ttime_metrics_f23 %>% 
  write.csv('data/cleaned/stations_routes_f23_travel_times_to_cbd.csv', row.names=FALSE)
ttime_metrics_f19 %>% 
  write.csv('data/cleaned/stations_routes_f19_travel_times_to_cbd.csv', row.names=FALSE)
```

# MBTA Rapid Transit Headways 

This is a very large dataset that would require us to:
- filter dates to our specific season
- calculate average or median headways per station (/direction?) for our specific season

2023 link: https://mbta-massdot.opendata.arcgis.com/datasets/896d043f33ae4915b530df12141c772c_0/explore
2019 link: https://mbta-massdot.opendata.arcgis.com/datasets/5c36245a6e7f4b89851b99a4a2bce02b_0/explore

For now, I think our current method of getting the headways from the schedules is adequate, but doing that with this dataset instead would be a nice-to-have since it'd measure actual headways and we could measure their variation as well.

# Walking-Distance Buffer Areas

This requires setting up an OpenRouteService API key (free) and loading it into your R environment as described here: https://giscience.github.io/openrouteservice-r/articles/openrouteservice.html 

```{r}
library(openrouteservice)

rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

Be careful about how many times you run this - the openrouteservice isochrones API limits you to 500 requests per day per API key. Each request returns 5 polygons, and we have ~150 stops, so a full run will use ~30 requests. There's also 20 request per minute throttling, so 1 full run per dataframe will take ~1.5 minutes.
```{r}
walk_polys <- function(rt_stations_df) {
  rt_stations_df %>%
    mutate( # API expects a df with these two col names and nothing else
      lon=stop_lon, lat=stop_lat, .keep='none'
      ) %>% 
    split(rep(1:ceiling(nrow(rt_stations_df)/5),each=5)) %>% # split into groups of 5
    lapply(function(df) {
      Sys.sleep(3.01) # pause in between requests to accommodate the 20 per minute limit
      ors_isochrones(df, profile=ors_profile(mode='walking'), range=600, output='sf')
      }) %>%
    bind_rows()
} 

walk_polys_f23 <- walk_polys(rt_stations_f23)
walk_polys_f19 <- walk_polys(rt_stations_f19)
```

This plot shows that many of the 10min walking polygons overlap, even on the same route. The next step addresses this potential violation of the independence assumption by converting these into polygons that don't overlap each other on the same routes.
```{r}
library(sf)

plot(st_geometry(walk_polys_f23))
```

Source for the following function: https://gis.stackexchange.com/questions/358797/splitting-overlap-between-polygons-and-assign-to-nearest-polygon-using-r 
```{r}
st_no_overlap <- function(polygons) {

  centroids <- st_centroid(polygons)

  # Voronoi tesselation
  voronoi <-
    centroids %>%
    st_geometry() %>%
    st_union() %>%
    st_voronoi() %>%
    st_collection_extract()

  # Put them back in their original order
  voronoi <- voronoi[unlist(st_intersects(centroids, voronoi))]

  # Keep the attributes
  result <- centroids

  # Intersect voronoi zones with polygons
  st_geometry(result) <-
    mapply(function(x, y) st_intersection(x, y),
           st_geometry(polygons),
           voronoi,
           SIMPLIFY = FALSE) %>%
    st_sfc(crs = st_crs(polygons))

  result
}
```

```{r}
walk_polys_f23[c('route_id', 'parent_station')] <- rt_stations_f23[c('route_id', 'parent_station')]
walk_polys_f19[c('route_id', 'parent_station')] <- rt_stations_f19[c('route_id', 'parent_station')]
```

```{r}
sf_use_s2(FALSE)

walk_polys_nonoverlap <- function(walk_polys_df) {
  walk_polys_df %>%
  st_transform(26986) %>% # convert to projected coord system before no_overlap calc
  group_split(route_id) %>% # split by route ID to limit the nonoverlap calculation to route-level
  lapply(function(df) st_no_overlap(df)) %>% 
  bind_rows() %>% # re-combine the list of dataframes by route into a single df
  st_transform(4326) # convert back to lat/long
}

walk_polys_f23_nonoverlap <- walk_polys_nonoverlap(walk_polys_f23)
walk_polys_f19_nonoverlap <- walk_polys_nonoverlap(walk_polys_f19)
```

Now, the plot shows the walk areas as nonoverlapping at the route level.
```{r}
plot(st_geometry(walk_polys_f23_nonoverlap))
```

Save the station areas with both an overlapping version and a non-overlapping version. 
```{r}
clean_walk_polys <- function(walk_polys_df) {
  walk_polys_df %>%
    mutate(walk_poly_wkt = st_as_text(geometry)) %>%
    st_drop_geometry() %>%
    select(route_id, parent_station, walk_poly_wkt)
}

walk_polys_f23 %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f23_geoms_overlapping.csv', row.names=FALSE)

walk_polys_f23_nonoverlap %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f23_geoms_nonoverlapping.csv', row.names=FALSE)

walk_polys_f19 %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f19_geoms_overlapping.csv', row.names=FALSE)

walk_polys_f19_nonoverlap %>% clean_walk_polys() %>%
  write.csv('data/cleaned/stations_routes_f19_geoms_nonoverlapping.csv', row.names=FALSE)
```

## CTPS park-and-ride inventory for MBTA stations

Downloaded from https://www.ctps.org/maploc/www/apps/pnr-dashboard/index.html and stored in the data folder.

```{r}
ctps <- read.csv('data/mbta/ctps_pnr_lots_polygons.csv')
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
ctps_clean <- ctps %>%
  filter(mode == 'Rapid Transit') %>%
  mutate(station = str_trim(station, side='both')) %>% # trim whitespaces from station names
  # and recode station names ad-hoc as needed to match our canonical rt_stations dataset
  mutate(station = case_when(station == 'Malden' ~ 'Malden Center', .default=station)) %>%
  # total spaces can be disaggregated by permit-only, handicap-only, and general public spots
  select(station, station_name, line_id, total_spaces_1) %>%
  group_by(station) %>%
  summarise(spaces = sum(total_spaces_1), .groups='drop')
```

```{r}
# I used these joins to figure out which station names needed to be recoded - they
# should have 0 rows. Any rows represent station names that didn't match successfully
ctps_clean %>%
  left_join(rt_stations_f23, by=join_by(station == stop_name)) %>%
  filter(is.na(route_id))

ctps_clean %>%
  left_join(rt_stations_f19, by=join_by(station == stop_name)) %>%
  filter(is.na(route_id))
```

```{r}
join_ctps <- function(rt_stations_df) {
  rt_stations_df %>%
  left_join(ctps_clean, by=join_by(stop_name == station)) %>%
  mutate(pnr_spaces = case_when(is.na(spaces) ~ 0, .default=spaces), .keep='unused') %>%
  select(route_id, parent_station, pnr_spaces)
}

rt_stations_f23 %>% join_ctps() %>% write.csv('data/cleaned/stations_routes_f23_pnr_spaces.csv', row.names=FALSE)
rt_stations_f19 %>% join_ctps() %>% write.csv('data/cleaned/stations_routes_f19_pnr_spaces.csv', row.names=FALSE)
```

# Walk Scores

```{r}
ws <- read.csv('data/rapid_trans_facilities_wlkscore.csv') # from Walk Score API via Ted
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
# these selections should have 0 rows. Any rows represent stations that didn't join to the walk scores
rt_stations_f23 %>%
  left_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  filter(is.na(walk_score))

rt_stations_f19 %>% select(-stop_name) %>%
  left_join(rt_stations_f23 %>% select(stop_name, route_id, parent_station), 
             by=c('route_id', 'parent_station')) %>%
  left_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  filter(is.na(walk_score))
```

The three missing-score stops from 2019 are the stops that were removed in the B branch consolidation project. Could be nice to get walk scores for those, but they can be removed for now

```{r}
rt_stations_f23 %>%
  inner_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  select(route_id, parent_station, walk_score) %>%
  write.csv('data/cleaned/stations_routes_f23_walk_scores.csv', row.names=FALSE)

rt_stations_f19 %>% select(-stop_name) %>%
  inner_join(rt_stations_f23 %>% select(stop_name, route_id, parent_station), 
             by=c('route_id', 'parent_station')) %>%
  inner_join(ws %>% select(stop_name, walk_score), by='stop_name') %>%
  select(route_id, parent_station, walk_score) %>%
  write.csv('data/cleaned/stations_routes_f19_walk_scores.csv', row.names=FALSE)
```

# Geography

For calculating whether each station is in Boston proper or not, as well as crows-fly distance from the central business district

```{r}
rt_stations_f23 <- read.csv('data/cleaned/stations_routes_f23.csv')
rt_stations_f19 <- read.csv('data/cleaned/stations_routes_f19.csv')
```

```{r}
library(sf)

bos <- st_read('data/boston_boundary/City_of_Boston_Boundary_(Water_Included).shp')
```

```{r}
calc_geog_metrics <- function(rt_stations_df) {
  station_points <- rt_stations_df %>%
    st_as_sf(coords = c("stop_lon","stop_lat"), crs=4326)
  
  station_points$in_bos <- station_points %>%
    st_intersects(bos %>% st_transform(4326)) %>%
    as.logical() %>% replace_na(0) %>% as.numeric()
  
  cbd <- st_sfc(st_point(c(-71.061418, 42.354919))) %>% 
    st_set_crs(4326)
  
  station_points$km_from_cbd <- station_points %>% st_transform(26986) %>%
    st_distance(cbd %>% st_transform(26986)) %>% as.numeric() / 1000
  
  station_points %>%
    st_drop_geometry() %>%
    select(-stop_name)
}

rt_stations_f23 %>% calc_geog_metrics() %>% 
  write.csv('data/cleaned/stations_routes_f23_geog_vars.csv', row.names=FALSE)
rt_stations_f19 %>% calc_geog_metrics() %>% 
  write.csv('data/cleaned/stations_routes_f19_geog_vars.csv', row.names=FALSE)
```

# College Locations & Enrollments (IPEDS)

2023 and 2019 data download links and data dictionaries available from https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx?gotoReportId=7&fromIpeds=true 

```{r}
# directory information - contains the names and coords of educational institutions
hd_23 <- read.csv('data/ipeds/hd2023.csv')
hd_19 <- read.csv('data/ipeds/hd2019.csv')

# 12-month enrollment per educational institution
effy_23 <- read.csv('data/ipeds/effy2023.csv')
effy_19 <- read.csv('data/ipeds/effy2019.csv')

# station-area geometries to which we will join the institution coordinates
rt_station_geoms_f23 <- read.csv('data/cleaned/stations_routes_f23_geoms_nonoverlapping.csv')
rt_station_geoms_f19 <- read.csv('data/cleaned/stations_routes_f19_geoms_nonoverlapping.csv')
```

```{r}
library(sf) 

enrollment_23 <- effy_23 %>%
  filter(EFFYALEV == 1) %>% # this code is for the "All students total" slice
  select(UNITID, EFYTOTLT) # this variable is the actual enrollment number

# annoyingly, the variable name changed slightly from EFFYLEV to EFFYALEV
enrollment_19 <- effy_23 %>%
  filter(EFFYLEV == 1) %>% # this code is for the "All students total" slice
  select(UNITID, EFYTOTLT) # this variable is the actual enrollment number

ma_college_data <- function(hd, enrollment) {
  hd %>% # this function filters to MA institutions & joins to their enrollments
    filter(STABBR == 'MA') %>%
    select(UNITID, INSTNM, LONGITUD, LATITUDE) %>%
    inner_join(enrollment, by='UNITID') %>%
    st_as_sf(crs=4326, coords = c("LONGITUD","LATITUDE")) %>%
    st_transform(crs=26986) # convert from lat-long to projected coord system for MA
}

ma_colleges_23 <- ma_college_data(hd_23, enrollment_23)
ma_colleges_19 <- ma_college_data(hd_19, enrollment_19)
```

```{r}
join_college_data <- function(rt_station_geoms, ma_college_df) {
  rt_station_geoms %>%
    st_as_sf(crs=4326, wkt = "walk_poly_wkt") %>%
    st_transform(crs=26986) %>%
    st_join(ma_college_df, join=st_intersects, left=TRUE) %>%
    
    # splits point enrollments evenly across intersecting station areas
    group_by(UNITID) %>%
    mutate(EFYTOTLT = EFYTOTLT/n()) %>%
    ungroup() %>%
  
    group_by(route_id, parent_station) %>% # sum up enrollments per station area
    summarise(college_students = sum(EFYTOTLT, na.rm=T), .groups='drop') %>%
    st_drop_geometry()
}

rt_station_geoms_f23 %>% 
  join_college_data(ma_colleges_23) %>%
  write.csv('data/cleaned/stations_routes_f23_colleges.csv', row.names=FALSE)

rt_station_geoms_f19 %>% 
  join_college_data(ma_colleges_19) %>%
  write.csv('data/cleaned/stations_routes_f19_colleges.csv', row.names=FALSE)
```

# Hotels & Hospitals

```{r}
# station-area geometries to which we will join the hotel coordinates
rt_station_geoms_f23 <- read.csv('data/cleaned/stations_routes_f23_geoms_nonoverlapping.csv')
rt_station_geoms_f19 <- read.csv('data/cleaned/stations_routes_f19_geoms_nonoverlapping.csv')
```

Hotels source: https://data.boston.gov/dataset/licensing-board-licenses/resource/04584beb-8160-4235-af57-bb8337efd512
Hospitals source: https://data.boston.gov/dataset/hospitals/resource/9ce5935a-bc4f-4a5c-a063-0f15fc01513a
```{r}
library(httr2) # to make the API request from data.boston.gov

bos_licenses_url <- 'https://data.boston.gov/api/3/action/datastore_search?resource_id=04584beb-8160-4235-af57-bb8337efd512&limit=5000'
bos_hospitals_url <- 'https://data.boston.gov/api/3/action/datastore_search?resource_id=9ce5935a-bc4f-4a5c-a063-0f15fc01513a&limit=5000'

licenses_api_raw <- req_perform(request(bos_licenses_url))
hospitals_api_raw <- req_perform(request(bos_hospitals_url))
```

```{r}
response_body <- function(api_raw) {
  api_raw %>%
  resp_body_string() %>%
  jsonlite::fromJSON()
}

licenses <- response_body(licenses_api_raw)$result$records
hospitals <- response_body(hospitals_api_raw)$result$records
```

Process and save hotels per route-station
```{r}
hotel_df <- licenses %>%
  filter(license_category == 'Inn', !is.na(gpsx)) %>%
  group_by(business_name, gpsx, gpsy) %>% 
  unique() %>% # deduplicate where multiple licenses exist for 1 business
  st_as_sf(coords = c("gpsx","gpsy"), crs=2249)

join_hotel_data <- function(rt_station_geoms) {
  rt_station_geoms %>%
    st_as_sf(crs=4326, wkt = "walk_poly_wkt") %>%
    st_transform(crs=2249) %>% # MA state plane system used by the license data
    st_join(hotel_df, join=st_intersects, left=TRUE) %>%
    group_by(route_id, parent_station) %>% # count hotels per station area
    summarise(hotels = sum(case_when(
        is.na(business_name) ~ 0, .default=1
      )), .groups='drop') %>%
    st_drop_geometry()
}

rt_station_geoms_f23 %>%
  join_hotel_data() %>%
  write.csv('data/cleaned/stations_routes_f23_hotels.csv', row.names=FALSE)
    
rt_station_geoms_f19 %>%
  join_hotel_data() %>%
  write.csv('data/cleaned/stations_routes_f19_hotels.csv', row.names=FALSE)
```

Process and save hospitals per route-station
```{r}
hospital_df <- hospitals %>%
  select(Name, POINT_X, POINT_Y, DailyAvg) %>%
  st_as_sf(coords = c("POINT_X", "POINT_Y"), crs=4326) %>%
  st_transform(crs=2249)

join_hospital_data <- function(rt_station_geoms) {
  rt_station_geoms %>%
    st_as_sf(crs=4326, wkt = "walk_poly_wkt") %>%
    st_transform(crs=2249) %>%
    st_join(hospital_df, join=st_intersects, left=TRUE) %>%
    group_by(route_id, parent_station) %>% # count hotels per station area
    summarise(
      hospitals = sum(case_when(
        is.na(Name) ~ 0, .default=1
      )), 
      hospital_capacity = sum(as.numeric(DailyAvg), na.rm=T), 
      .groups='drop') %>%
    st_drop_geometry()
}

rt_station_geoms_f23 %>%
  join_hospital_data() %>%
  write.csv('data/cleaned/stations_routes_f23_hospitals.csv', row.names=FALSE)
    
rt_station_geoms_f19 %>%
  join_hospital_data() %>%
  write.csv('data/cleaned/stations_routes_f19_hospitals.csv', row.names=FALSE)
```