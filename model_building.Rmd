---
title: "Model Building"
author: "Emmett Greenberg, Ted Banken, Ethan McIntosh"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse) # for pipe notation and tabular functions
library(sf) # for geographic operations
library(car) # for the vif function to test multicollinearity
library(leaflet) # for mapping
library(leaflegend) # for extra map legend functions
library(ape) # for spatial autocorrelation (Moran's I) function
```

This script will be where we build & validate our models of ridership.

## Read in and join the data

```{r}
get_dataset <- function(dataset) {
  read.csv(paste0('data/cleaned/stations_routes_', dataset, '.csv'))
}

id_cols <- c('route_id', 'parent_station') # most of our data are at the route_station level
xfer_station_names <- c('Park Street', 'Downtown Crossing', 'Government Center', 'State')

get_model_data <- function(season) {
  get_dataset(season) %>%
    inner_join(get_dataset(paste(season, 'geog_vars', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'bus&CR_connections', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'headways', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'spacings', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'travel_times_to_cbd', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'pnr_spaces', sep='_')), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'walk_scores', sep='_')), by=id_cols) %>%
    inner_join(get_dataset('acs2022_5yr_avgMedianIncome'), by=id_cols) %>%
    inner_join(get_dataset('acs2022_5yr_vehicles_available'), by=id_cols) %>%
    inner_join(get_dataset('acs2022_5yr_pop'), by=id_cols) %>%
    inner_join(get_dataset('2021_wac_agg'), by=id_cols) %>%
    inner_join(get_dataset('2024_lu_entropy'), by=id_cols) %>%
    inner_join(get_dataset('airbnbs_2024_bos_cam'), by=id_cols) %>%
    inner_join(get_dataset(paste(season, 'colleges', sep='_')), by=id_cols) %>%  
    inner_join(get_dataset(paste(season, 'hotels', sep='_')), by=id_cols) %>% 
    inner_join(get_dataset(paste(season, 'hospitals', sep='_')), by=id_cols) %>% 
    left_join(get_dataset(paste(season, 'terminals', sep='_')), by=id_cols) %>%
    left_join(get_dataset(paste(season, 'ridership', sep='_')), by=id_cols) %>%
    mutate( # this first mutate is for any operations where we want to keep the variables that are used
      transfer = case_when(stop_name %in% xfer_station_names ~ 1, .default=0),
      airbnb_count = case_when(is.na(airbnb_count) ~ 0, .default=airbnb_count),
      cov_tt = sd_tt/avg_tt # coefficient of variation of travel time to CBD
    ) %>%
    mutate( # and this second mutate is for any where we can discard the variables that are used
      terminal = case_when(terminal == 1 ~ 1, .default=0),
      devl_mix = tot_jobs/(tot_pop + tot_jobs), 
      hh_no_vehicle_share = total_HH_no_vehicle / total_HH,
      avg_veh_per_hh = (total_HH_one_vehicle + 2*total_HH_two_vehicle + 
                          3*total_HH_three_vehicle + 4*total_HH_four_vehicle) / total_HH,
      jobs_per_acre = mean_jobs_sqft*43560,
      pop_per_acre = ppsqft*43560, 
      college_students_1000s = college_students/1000,
      connecting_bus_riders_100s = connecting_bus_riders/100,
      median_inc_1000s = avg_median_inc/1000, 
      pnr_spaces_100s = pnr_spaces/100,
      land_use_entropy_score = land_use_entropy*100,
      avg_trav_time_to_cbd = avg_tt,
      .keep='unused'
      ) %>%
    mutate(route=1, route_id_temp = route_id) %>% # this creates dummy variables for each route
    pivot_wider(names_from='route_id_temp', values_from='route', values_fill=0)
}

model_data_f23 <- get_model_data('f23')
model_data_f19 <- get_model_data('f19')
```

## Descriptive Statistics

```{r}
continuous_vars <- c('avg_boardings', 'avg_headway_wkdy', 'avg_headway_wknd', 
                     'avg_trav_time_to_cbd', 'cov_tt', 'avg_spacing_km', 'walk_score', 
                     'km_from_cbd', 'median_inc_1000s', 'pop_per_acre', 'jobs_per_acre', 
                     'devl_mix', 'hh_no_vehicle_share', 'avg_veh_per_hh', 'airbnb_count')
sparse_vars <- c('connecting_bus_riders_100s', 'connecting_cr_routes',
                 'pnr_spaces_100s', 'college_students_1000s')
dummy_vars <- c('Red', 'Orange', 'Green', 'Blue', 'transfer', 'terminal', 'in_bos')


numeric_data <- model_data_f23 %>% select(all_of(c(continuous_vars, sparse_vars)))
summary(numeric_data) # minimum, median, mean, maximum, and interquartile range for each variable
```

For dummy variables (0 or 1), show what number and percent of observations the 1's represent
```{r}
model_data_f23 %>% 
  summarise(across(all_of(dummy_vars), list(num_1s = sum, pct_1s = mean))) %>%
  t()
```

For sparse variables, show what number and percent of observations the 0's represent
```{r}
sum_zeroes <- function(x) {
  sum(x==0)
}

pct_zeroes <- function(x) {
  sum(x==0)/length(x)
}

model_data_f23 %>% 
  summarise(across(all_of(sparse_vars), list(num_0s = sum_zeroes, pct_0s = pct_zeroes))) %>%
  t()
```

Histograms of continuous variables
```{r}
continuous_vars %>% 
  lapply(function(var) hist(
    model_data_f23[[var]], breaks='Sturges', 
    main=paste('Histogram of', var), xlab=paste('Value of', var)
    )) %>% invisible()
```

Histograms of non-zero values for sparse variables (variables with mostly 0s)
```{r}
sparse_vars %>%
  lapply(function(var) hist(
    model_data_f23[[var]] %>% {.[. != 0]}, breaks='Sturges', 
    main=paste('Histogram of', var), xlab=paste('Value of', var)
    )) %>% invisible()
```

Pairs plot
```{r}
model_data_f23 %>% select(all_of(continuous_vars)) %>% pairs()
```

Correlation matrix
```{r}
model_data_f23 %>% select(all_of(continuous_vars)) %>% cor()
```

## Model Building

```{r}
# these functions spit out regressions for a given dataframe and dependent variable name,
# specified through a list of named parameters "data" and "y"

base_reg_full <- function(params) {
  eval(bquote(
    lm(.(as.symbol(paste0('avg_boardings_', params$daytype))) ~ avg_trav_time_to_cbd +
       avg_headway + avg_spacing_km + cov_tt
       + Red + Orange + Blue + transfer + terminal #+ km_from_cbd
       + connecting_cr_routes + connecting_bus_routes + pnr_spaces_100s
       + walk_score + land_use_entropy_score + walk_score*land_use_entropy_score 
       + pop_per_acre + jobs_per_acre + median_inc_1000s #+ avg_veh_per_hh
       , data=params$data %>% 
         rename(avg_headway = .(as.symbol(paste0('avg_headway_', params$daytype))))
     )))
}

base_reg_bosonly <- function(params) {
  params %>% base_reg_full() %>% update(. ~ . -Red -Orange -Blue -jobs_per_acre)
}

enhanced_reg_full <- function(params) { 
  params %>% base_reg_full() %>% update(. ~ . +college_students_1000s)
}

enhanced_reg_bosonly <- function(params) { 
  params %>% base_reg_bosonly() %>% update(. ~ . +college_students_1000s +hotels +hospitals +airbnb_density)
}

specifications <- list(
  # "Fall 2019 am peak" = list(data=model_data_f19, daytype='wkdy', period='am_peak'), 
  # "Fall 2023 am peak" = list(data=model_data_f23, daytype='wkdy', period='am_peak'),
  # "Fall 2019 off peak" = list(data=model_data_f19, daytype='wkdy', period='off_peak'), 
  # "Fall 2023 off peak" = list(data=model_data_f23, daytype='wkdy', period='off_peak'),
  # "Fall 2019 pm peak" = list(data=model_data_f19, daytype='wkdy', period='pm_peak'), 
  # "Fall 2023 pm peak" = list(data=model_data_f23, daytype='wkdy', period='pm_peak'),
  # "Fall 2019 weekend" = list(data=model_data_f19, daytype='wknd', period='wknd'), 
  # "Fall 2023 weekend" = list(data=model_data_f23, daytype='wknd', period='wknd')
  "Fall 2019 weekday" = list(data=model_data_f19, daytype='wkdy'),
  "Fall 2023 weekday" = list(data=model_data_f23, daytype='wkdy'),
  "Fall 2019 weekend" = list(data=model_data_f19, daytype='wknd'),
  "Fall 2023 weekend" = list(data=model_data_f23, daytype='wknd')
)

specifications_bosonly <- specifications %>% 
  lapply(function(s) {list(data=s$data %>% filter(in_bos == 1), daytype=s$daytype, period=s$period)})

base_full_regs <- specifications %>% lapply(base_reg_full) %>%
  `names<-`(names(.) %>% lapply(function(name) paste(name, "base full")))
enhanced_full_regs <- specifications %>% lapply(enhanced_reg_full) %>%
  `names<-`(names(.) %>% lapply(function(name) paste(name, "enhanced full")))

base_bosonly_regs <- specifications_bosonly %>% lapply(base_reg_bosonly) %>%
  `names<-`(names(.) %>% lapply(function(name) paste(name, "base bosonly")))
enhanced_bosonly_regs <- specifications_bosonly %>% lapply(enhanced_reg_bosonly) %>%
  `names<-`(names(.) %>% lapply(function(name) paste(name, "enhanced bosonly")))

regressions <- base_full_regs %>% c(enhanced_full_regs) %>% 
  c(base_bosonly_regs) %>% c(enhanced_bosonly_regs)
summaries <- regressions %>% lapply(summary)

coefficients <- summaries %>% lapply(function(reg) reg$coefficients)
adj_r2s <- summaries %>% lapply(function(reg) reg$adj.r.squared)
f_stats <- summaries %>% lapply(function(reg) reg$fstatistic)
rmses <- regressions %>% lapply(function(reg) sqrt(mean(reg$residuals^2)))

summaries[['Fall 2023 weekday base full']] # display summary for a single specification
```

Create a formatted table for easier comparison across specifications
```{r}
rc <- function(n, digits) round(n, digits) %>% as.character()

format_results <- function(result, vif) {
  out_str <- result['Estimate'] %>%
    rc(2) %>%
    paste(paste0(
      '(se=', result['Std. Error'] %>% rc(2)))
  
  if (!is.na(vif)) {
      out_str <- out_str %>% paste0(', vif=', vif %>% rc(2))
  }
  
  out_str <- out_str %>% paste0(', t=', result['t value'] %>% rc(2), ')')
  out_str
}

compare_specifications <- function(var) {
  names(coefficients) %>% 
    lapply(function(spec) {
      if (var %in% row.names(coefficients[[spec]])) {
        vif <- NA
        if (var %in% names(vif(regressions[[spec]]))) {
          vif <- vif(regressions[[spec]])[[var]]
        }
        format_results(coefficients[[spec]][var,], vif)
      }
      else '--'
    }) %>% 
    `names<-`(names(coefficients)) %>% 
    as.data.frame()
}

var_names <- do.call(c, coefficients %>% lapply(row.names)) %>% unique() # get all variable names

formatted_output <- var_names %>%
  lapply(compare_specifications) %>%
  bind_rows() %>%
  `row.names<-`(var_names) %>%
  bind_rows(
    adj_r2s %>% 
      lapply(function(ar2) rc(ar2, 3)) %>% 
      as.data.frame() %>% 
      `row.names<-`('Adjusted R Squared'),
    f_stats %>% 
      lapply(function(ar2) rc(ar2, 3)) %>% 
      as.data.frame() %>% 
      `row.names<-`(c('F Statistic', 'df_vars', 'df_obs')),
    rmses %>% 
      lapply(function(ar2) rc(ar2, 3)) %>% 
      as.data.frame() %>% 
      `row.names<-`('RMSE')
    )

formatted_output
```

Write the formatted output to csv if we like it
```{r}
formatted_output %>% write.csv('formatted_output.csv', row.names = TRUE)
```


Predicted vs Actual plots

```{r}
plot_y_yhat_w_labels <- function(df, reg, add_labels=FALSE, transformation=I, transform_prefix='') {
  yname <- names(reg$model)[[1]]
  y_actual <- df[[yname]] %>% transformation()
  yhat <- predict(reg) %>% transformation()
  resid <- reg$residuals
  
  plot(x=y_actual, y=yhat, pch=16, col=df$route_id, # use route IDs as point colors
       xlab=paste(transform_prefix, 'Actual Boardings'),
       ylab=paste(transform_prefix, 'Predicted Boardings'),
       main=paste(transform_prefix, 'Predicted vs. Actual Boardings by Route-Station'))
  abline(a=0, b=1) # draw 1=1 line for reference
  legend("bottomright", title="MBTA Route", pch=16,
         legend=c("Red", "Blue", "Orange", "Green"), col=c("red", "blue", "orange", "green"))
  
  # option to add station labels to help identify the kinds of places we're over- or under-predicting
  if (add_labels) {
    df_large_resid <- df[abs(resid) > 3000,]
    y_large_resid <- df_large_resid[[yname]] %>% transformation()
    yhat_large_resid <- yhat[abs(resid) > 3000] #%>% transformation()
    text(x=y_large_resid, y=yhat_large_resid, adj=c(0.5,1.5), cex=0.5,
       labels=paste0(
         str_sub(df_large_resid$route_id, end=1),
         '-', str_sub(df_large_resid$stop_name, end=3))
       )
    
  }
}
```

```{r}
specifications[['Fall 2023 weekday']]$data %>%
  plot_y_yhat_w_labels(regressions[['Fall 2023 weekday base full']], add_labels=F)

specifications[['Fall 2023 weekday']]$data %>%
  plot_y_yhat_w_labels(
    regressions[['Fall 2023 weekday base full']], #add_labels=T, 
    transformation=log, transform_prefix='Logged ')
```

Actual vs predicted map of stations

```{r, message=FALSE}
pal <- colorFactor(
  palette = c('yellow', 'purple'), # black for overprediction, white for underprediction
  domain = c("over-prediction", "under-prediction")
)

mdf <- model_data_f23 %>%
  mutate(residual = regressions[['Fall 2023 weekday base full']]$residuals %>% as.numeric(),
         residual_pct = residual/avg_boardings_wkdy
         #residual_scaled = rescale(abs(residual), c(2,10))
         ) %>%
  select(route_id, parent_station, stop_name, stop_lat, stop_lon, residual, residual_pct) %>%
  st_as_sf(coords = c("stop_lon","stop_lat"), crs=4326) 

mdf %>%
  leaflet() %>% addProviderTiles(providers$CartoDB.Positron) %>% setView(-71.1, 42.3, zoom=11) %>%
  addSymbolsSize(
    values = ~abs(residual),
    stroke = TRUE, color="gray", # marker outline
    fillColor=~pal(ifelse(residual>0, "under-prediction", "over-prediction")), 
    fillOpacity=0.6,
    baseSize = 5, shape='circle'
  ) %>%
  addLegendSize( # legend for the point colors
    position="bottomleft",
    values = ~abs(residual),
    color = 'gray',
    fillColor = 'gray',
    fillOpacity = .6,
    shape='circle',
    title = 'Error Size (Weekday Riders)',
    orientation = 'horizontal',
    breaks = 4,
    baseSize = 5) %>%
  addLegend( # legend for the point size
    "bottomleft", pal = pal, title = "Error Type", opacity = 1,
    values = ~ifelse(residual>0, "under-prediction", "over-prediction")
  ) %>%
  addScaleBar()
```

```{r}
hist(regressions[['Fall 2023 weekday base full']]$residuals, main="Distribution of Model Residuals")
```

We want the residuals to be normally distributed.

Spatial autocorrelation test, using inverse distance weights
```{r}
inv_dists <- 1 / as.matrix(dist(cbind(model_data_f23$stop_lon, model_data_f23$stop_lat)))
inv_dists[!is.finite(inv_dists)] <- 0

midf <- model_data_f23 %>%
  mutate(residual = regressions[['Fall 2023 weekday base full']]$residuals %>% as.numeric())

Moran.I(midf$residual, inv_dists)
```

The null hypothesis that there is no spatial autocorrelation, which is what we want for the residuals.